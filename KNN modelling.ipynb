{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,precision_score,recall_score,f1_score,plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"E:\\Andrei workspace\\Milestone project-1\\heart-disease.csv\")\n",
    "knn=KNeighborsClassifier()\n",
    "X=df.drop('target',axis=1)\n",
    "y=df['target']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 924 candidates, totalling 2772 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "462 fits failed out of a total of 2772.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "462 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"c:\\Users\\Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 437, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"c:\\Users\\Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 374, in _check_algorithm_metric\n",
      "    raise ValueError(\n",
      "ValueError: Metric 'Euclidean' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['brute']) to get valid options. Metric can also be a callable function.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.70231481 0.70231481\n",
      " 0.71481481 0.71481481 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.669393   0.669393   0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68184156 0.68184156 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70231481 0.70231481 0.71481481 0.71481481 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.669393   0.669393   0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68184156 0.68184156 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.70231481 0.70231481 0.71481481 0.71481481\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.669393   0.669393\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68184156 0.68184156\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535\n",
      " 0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.70231481 0.70231481\n",
      " 0.71481481 0.71481481 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.669393   0.669393   0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68184156 0.68184156 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70231481 0.70231481 0.71481481 0.71481481 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.669393   0.669393   0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68184156 0.68184156 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.70231481 0.70231481 0.71481481 0.71481481\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.669393   0.669393\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68184156 0.68184156\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535\n",
      " 0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.70231481 0.70231481\n",
      " 0.71481481 0.71481481 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.669393   0.669393   0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68184156 0.68184156 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70231481 0.70231481 0.71481481 0.71481481 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.669393   0.669393   0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68184156 0.68184156 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.70231481 0.70231481 0.71481481 0.71481481\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.669393   0.669393\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68184156 0.68184156\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535\n",
      " 0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.69819959 0.69819959\n",
      " 0.71893004 0.71893004 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.68179012 0.68179012 0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68595679 0.68595679 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69819959 0.69819959 0.71893004 0.71893004 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.68179012 0.68179012 0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68595679 0.68595679 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.69819959 0.69819959 0.71893004 0.71893004\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.68179012 0.68179012\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68595679 0.68595679\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535\n",
      " 0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.69819959 0.69819959\n",
      " 0.71893004 0.71893004 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.68179012 0.68179012 0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68595679 0.68595679 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69819959 0.69819959 0.71893004 0.71893004 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.68179012 0.68179012 0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68595679 0.68595679 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.69819959 0.69819959 0.71893004 0.71893004\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.68179012 0.68179012\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68595679 0.68595679\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535\n",
      " 0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.69819959 0.69819959\n",
      " 0.71893004 0.71893004 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.68179012 0.68179012 0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68595679 0.68595679 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69819959 0.69819959 0.71893004 0.71893004 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.68179012 0.68179012 0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68595679 0.68595679 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.69819959 0.69819959 0.71893004 0.71893004\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.68179012 0.68179012\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68595679 0.68595679\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535\n",
      " 0.66095679 0.66095679 0.66929012 0.66929012 0.67335391 0.67335391\n",
      " 0.64855967 0.64855967 0.66095679 0.66095679 0.65694444 0.65694444\n",
      " 0.65694444 0.65694444 0.66100823 0.66100823 0.656893   0.656893\n",
      " 0.6651749  0.6651749  0.66100823 0.66100823 0.69819959 0.69819959\n",
      " 0.71893004 0.71893004 0.71893004 0.71893004 0.69830247 0.69830247\n",
      " 0.68179012 0.68179012 0.66949588 0.66949588 0.669393   0.669393\n",
      " 0.68595679 0.68595679 0.66929012 0.66929012 0.67340535 0.67340535\n",
      " 0.67340535 0.67340535        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69819959 0.69819959 0.71893004 0.71893004 0.71893004 0.71893004\n",
      " 0.69830247 0.69830247 0.68179012 0.68179012 0.66949588 0.66949588\n",
      " 0.669393   0.669393   0.68595679 0.68595679 0.66929012 0.66929012\n",
      " 0.67340535 0.67340535 0.67340535 0.67340535 0.64449588 0.64449588\n",
      " 0.65694444 0.65694444 0.656893   0.656893   0.65272634 0.65272634\n",
      " 0.6651749  0.6651749  0.65282922 0.65282922 0.65277778 0.65277778\n",
      " 0.62391975 0.62391975 0.63621399 0.63621399 0.62803498 0.62803498\n",
      " 0.63631687 0.63631687 0.69819959 0.69819959 0.71893004 0.71893004\n",
      " 0.71893004 0.71893004 0.69830247 0.69830247 0.68179012 0.68179012\n",
      " 0.66949588 0.66949588 0.669393   0.669393   0.68595679 0.68595679\n",
      " 0.66929012 0.66929012 0.67340535 0.67340535 0.67340535 0.67340535]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=KNeighborsClassifier(),\n",
       "             param_grid={'leaf_size': array([ 8,  9, 10, 11, 12, 13, 14]),\n",
       "                         'metric': ['cosine', 'manhattan', 'Euclidean', 'l1',\n",
       "                                    'l2', 'cityblock'],\n",
       "                         'n_neighbors': array([17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47]),\n",
       "                         'p': [1, 2]},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_knn_grid={'n_neighbors':np.arange(17,50,3),'leaf_size':np.arange(8,15),'p':[1,2],'metric':['cosine','manhattan','Euclidean', 'l1','l2', 'cityblock']}\n",
    "gs_knn=GridSearchCV(knn,param_grid=baseline_knn_grid,cv=3,verbose=True)\n",
    "gs_knn.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gs_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7189300411522633,\n",
       " {'leaf_size': 11, 'metric': 'manhattan', 'n_neighbors': 20, 'p': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_knn.best_score_,gs_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "X_train_scaled=pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled=pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1=KNeighborsClassifier(leaf_size= 11, metric= 'manhattan', n_neighbors= 20, p= 1)\n",
    "knn1.fit(X_train_scaled,y_train)\n",
    "knn1.score(X_test_scaled,y_test)\n",
    "y_pred_scaled=knn1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64        29\n",
      "           1       0.68      0.72      0.70        32\n",
      "\n",
      "    accuracy                           0.67        61\n",
      "   macro avg       0.67      0.67      0.67        61\n",
      "weighted avg       0.67      0.67      0.67        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.72      0.78        29\n",
      "           1       0.78      0.88      0.82        32\n",
      "\n",
      "    accuracy                           0.80        61\n",
      "   macro avg       0.81      0.80      0.80        61\n",
      "weighted avg       0.81      0.80      0.80        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ace90a754af0e77d47ef115ea43993774ca2fb93d38a6af477c7ca92fb1e3f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
